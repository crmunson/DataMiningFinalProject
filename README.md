# DataMiningFinalProject - Connor Munson, Hugh Frampton, Josh Aney, Owen Cool


For our project, we aim to predict the likelihood of heart disease in patients, given patient information such as age, resting blood pressure, cholesterol, and maximum heart rate. Further, we hope to identify potential “clusters” of patients exhibiting similar characteristics on a variety of features, and predictive PCA-derived features that may have better explanatory power in predicting rates of heart disease than raw data alone. For this, we will use the UCI Machine Learning Repository’s Heart Disease dataset.
There are many stakeholders who stand to benefit from the completion of this project. Heart Disease is the leading cause of death in the United States, and gaining more insight into potential predictors of heart disease or at-risk patient clusters could be incredibly useful in preventing and treating heart disease, producing positive outcomes for millions of patients. Similarly, doctors stand to benefit by having increased insight which may be useful in further research into heart disease, or in faster, more accurate diagnosis of heart disease. While not a focus of this project, later work could investigate how inequities across different demographics and socioeconomic backgrounds may be linked to different rates of heart disease.
The UCI Machine Learning Repository’s Heart Disease dataset includes a total of 303 instances (patients), each with thirteen features, not including a single target variable (diagnosis of heart disease - a numerical variable ranging from 0-4). Of these, seven are categorical and six are numerical. The seven categorical variables are the sex of the patient, the type of chest pain the patient is experiencing, whether the patient’s fasting blood sugar is above 120 mg/dl, the patient’s resting electrocardiographic results, whether the patient has exercise induced angina, the slope of the peak exercise ST segment, and whether the patient has thalassemia. The six numerical variables are the patient’s age, the patient’s resting blood pressure, the patient’s serum cholesterol, the maximum heart rate achieved by the patient, the patient’s ST depression induced by exercise relative to rest, and the number of major vessels colored by fluoroscopy. Our data mining techniques will use the 13 features to cluster patients and predict their diagnoses. There are some missing values, though, for variables indicating the number of major vessels colored by fluoroscopy, and whether the patient has thalassemia. However, the number of missing values is limited and should not substantially impact the quality of our results.
This project will use a variety of different data mining techniques to help us solve the problem that we discussed earlier as well as helping us learn more about the data for the future. First, there will be a large data preprocessing step. This will include handling missing values which will be done by removing specific instances if there are few enough cases or by completing data imputation if needed. We will also perform one-hot encoding for all categorical attributes. After preprocessing is complete, we will perform feature selection if we find necessary. If there are no features that could be feasibly removed, we will perform dimensionality reduction through principal component analysis. Finally, we will use different clustering and classification techniques such as K-Means, DBSCAN, and Naive Bayes to address the problem stated above.
To evaluate our results, we will use a combination of a modified precision metric, raw precision, and accuracy to understand how our models are performing. For clustering, the modified precision metric will be used to allow us to analyze our model without comparing directly to ground truth. For our classification models, we will use a train-test split technique to set up and train our models. Once this is done, we will use raw precision and accuracy to assess the model’s performance. We will also provide confusion matrices to fully understand the model’s behavior. This all will give us insight on the data as well as the problems that we are trying to solve.
There may be some issues that might arise when we are doing the analysis of this dataset. If we find that the data is non-linear, we will need to use kernel principal component analysis for dimensionality reduction. We also may have to choose different classification models such as a neural network depending on how Naive Bayes performs. If the situation arises, this will need to be postponed to the future because of the time constraints of this project.
The group members for this project are Josh Aney, Connor Munson, Owen Cool, and Hugh Frampton.
